{
  "metadata": {
    "name": "Examples",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.ssql\n-- \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d Kinesis \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n-- https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/table/connectors/kinesis.html\n\n-- CREATE TABLE `my_table` (\n--   `column1` STRING,\n--   `column2` BIGINT\n-- )\n-- WITH (\n--   \u0027connector\u0027 \u003d \u0027kinesis\u0027,\n--   \u0027stream\u0027 \u003d \u0027my_kinesis_stream\u0027,\n--   \u0027aws.region\u0027 \u003d \u0027kinesis-stream-aws-region\u0027,\n--   \u0027scan.stream.initpos\u0027 \u003d \u0027LATEST\u0027,\n--   \u0027format\u0027 \u003d \u0027csv\u0027\n-- );\n\n\n-- \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d Kafka \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n-- https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/connectors/kafka.html\n\n-- Option 1: Plaintext endpoint\nDROP TABLE IF EXISTS statefun_egress;\nCREATE TABLE statefun_egress (\n  `event_time` TIMESTAMP(3) METADATA FROM \u0027timestamp\u0027,\n  `partition` BIGINT METADATA VIRTUAL,\n  `offset` BIGINT METADATA VIRTUAL,\n  `message` STRING\n) WITH (\n  \u0027connector\u0027 \u003d \u0027kafka\u0027,\n  \u0027topic\u0027 \u003d \u0027greeter-egress\u0027,\n  \u0027properties.bootstrap.servers\u0027 \u003d \u0027{Bootstrap servers}\u0027,\n  \u0027properties.group.id\u0027 \u003d \u0027myGroup\u0027,\n  \u0027scan.startup.mode\u0027 \u003d \u0027earliest-offset\u0027,\n  \u0027format\u0027 \u003d \u0027raw\u0027\n);\n\n\n-- Option 2: IAM endpoint \n-- Note: Please use this option if you are using MSK Serverless - https://aws.amazon.com/msk/features/msk-serverless/\n-- https://github.com/aws/aws-msk-iam-auth#configuring-a-kafka-client-to-use-aws-iam\n\n-- CREATE TABLE my_table (\n--   `column1` STRING,\n--   `column2` BIGINT\n-- ) WITH (\n--   \u0027connector\u0027 \u003d \u0027kafka\u0027,\n--   \u0027topic\u0027 \u003d \u0027my_topic\u0027,\n--   \u0027properties.bootstrap.servers\u0027 \u003d \u0027kafka_broker_endpoint:9098\u0027,\n--   \u0027properties.security.protocol\u0027 \u003d \u0027SASL_SSL\u0027,\n--   \u0027properties.sasl.mechanism\u0027 \u003d \u0027AWS_MSK_IAM\u0027,\n--   \u0027properties.sasl.jaas.config\u0027 \u003d \u0027software.amazon.msk.auth.iam.IAMLoginModule required;\u0027,\n--   \u0027properties.sasl.client.callback.handler.class\u0027 \u003d \u0027software.amazon.msk.auth.iam.IAMClientCallbackHandler\u0027,\n--   \u0027properties.group.id\u0027 \u003d \u0027myGroup\u0027,\n--   \u0027scan.startup.mode\u0027 \u003d \u0027earliest-offset\u0027,\n--   \u0027format\u0027 \u003d \u0027json\u0027\n-- );"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.pyflink\n\npip install confluent_kafka"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.pyflink\n\nfrom confluent_kafka import Producer\n\nuser_id \u003d \u0027Bob\u0027\npayload \u003d \u0027{\"name\": \"Bob\"}\u0027\n\np \u003d Producer({\u0027bootstrap.servers\u0027: \u0027{Bootstrap servers}\u0027})\np.produce(\n    topic\u003d\u0027greeter-ingress\u0027,\n    key\u003duser_id.encode(\u0027utf-8\u0027),\n    value\u003dpayload\n)\n\np.flush()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.ssql(type\u003dupdate)\n\nselect * from statefun_egress;"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.ssql\n"
    }
  ]
}